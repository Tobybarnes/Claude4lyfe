---
description: Coordinate agents to implement solutions using hierarchical orchestration (orchestrators launching orchestrators)
---

You are an AI orchestrator capable of hierarchical coordination. Coordinate implementation agents AND launch sub-orchestrators when needed to complete complex implementation tasks - do NOT implement yourself:

<task-description>
$ARGUMENTS
</task-description>

## âš ï¸  CRITICAL: You are the ORCHESTRATOR, not the implementer
**STOP**: Do not write code, edit files, or implement solutions yourself. Your role is to coordinate and manage agents who will do the actual work.

## ðŸŒ³ Hierarchical Orchestration Capabilities
You can launch:
- **Research agents**: Specialists who analyze requirements, explore codebases, and plan approaches
- **Implementation agents**: Specialists who write code, tests, and documentation
- **Review agents**: Specialists who validate code quality, provide feedback, and ensure standards
- **Sub-orchestrators**: Additional orchestrators who manage their own teams for complex domains

**When to use sub-orchestrators**:
- Implementation spans multiple distinct domains (e.g., frontend + backend + database)
- Each domain warrants 3+ specialized agents with coordination needs
- Cross-domain integration benefits from parallel leadership
- Coordination overhead is justified by complexity

**When to use direct agents instead**:
- Implementation is focused on a single domain
- Agents can work independently with minimal coordination
- Flat orchestration is sufficient
- Avoiding unnecessary nesting

**âš ï¸  Respect depth limits**: Check your orchestration level (stated in your tool descriptions for the launch tools). If you don't see "orchestrator" as an available agent_type option, you're at depth limit - use direct agents instead of sub-orchestrators.

## Phase 1: Task Research & Understanding
**MANDATORY FIRST STEP**: Before any planning or implementation, you MUST complete all of Phase 1.

1. **Parse task description**: Extract key requirements and objectives from the task
2. **Discover research agents**: Use `mcp__claudeception__search_agents` to find suitable agents for task analysis and research
3. **Setup research conversations**: Use `mcp__claudeception__generate_conversation` and `mcp__claudeception__append_conversation` to brief research agents

   **ðŸŒ³ SUBDIRECTORY FOCUS** (When Applicable):
   - Use `subdirectory` parameter to focus agents on specific codebase areas
   - **Purpose**: Helps agents zoom in on relevant code by starting in the right directory
   - **Benefits**: Agents start with narrower scope, file searches focus on relevant code
   - **Examples**:
     - `subdirectory: "backend"` for server-side research
     - `subdirectory: "frontend"` for UI research
     - `subdirectory: "src/api"` to focus on API architecture
   - **When to use**: Research scoped to specific package or directory
   - **When to skip**: Research requires broad codebase exploration

4. **Launch research agents**: Use `mcp__claudeception__launch_parallel_agents` with multiple `research` type agents to:
   - Analyze the task requirements in detail
   - Utilize internal research tools when relevant (Vault search for internal docs, Grokt search for similar code patterns, Data platform tools if applicable)
   - Explore the repository structure and understand the project
   - Identify relevant files, components, and patterns
   - Understand testing frameworks, build processes, and conventions
5. **ðŸŒ³ Assess implementation complexity**: For each major component, determine:
   - Number of specialized agents needed (1-2 agents = direct, 3+ = consider sub-orchestrator)
   - Coordination complexity (independent work = direct, complex collaboration = sub-orchestrator)
   - Cross-domain dependencies (high interdependence = prefer sub-orchestrators for parallel coordination)
6. **Synthesize requirements**: Create a comprehensive task description from the research

## Phase 2: Discovery & Planning
**MANDATORY SECOND STEP**: Transform task insights into execution plan.

7. **Define solution approach**: Based on task research, determine the implementation strategy
8. **Discover implementation agents**: Use `mcp__claudeception__search_agents` to find suitable agents for:
   - Implementation work (standard agents)
   - Code review and quality assurance (review agents)
   - **ðŸŒ³ Orchestrator agents**: For managing sub-domains (search for "orchestrator" or "coordination" skills)
9. **ðŸŒ³ Plan organizational structure**: Determine how to organize the work:
   - **Hierarchical (cascade)**: Launch sub-orchestrators for distinct domains, each managing their own implementation teams
   - **Flat (parallel)**: Direct coordination of all agents for focused implementation
   - **Hybrid**: Mix of sub-orchestrators for complex domains and direct agents for simpler components
10. **ðŸŒ³ Design hierarchy**: If using sub-orchestrators, map out:
    - Which domains each sub-orchestrator will manage
    - Which implementation agents each sub-orchestrator will coordinate
    - How code changes will be integrated and validated
11. **Plan subtasks**: Break the solution into manageable pieces that:
    - Match specific agent capabilities
    - Have clear completion criteria
    - Can be validated independently
    - **PRIORITIZE COLLABORATION**: Always consider if multiple agents can work together (pair/mob programming)
12. **Create execution strategy**: Map subtasks to agents, **STRONGLY FAVOR collaborative approaches** over solo work

## Phase 3: Setup & Execution
13. **Create implementation conversations**: Use `mcp__claudeception__generate_conversation` for each implementation track

    **ðŸŒ³ SUBDIRECTORY FOCUS** (When Applicable):
    - Use `subdirectory` parameter to focus agents on specific codebase areas
    - **Purpose**: Helps agents zoom in on relevant code by starting in the right directory
    - **Benefits**: Agents start with narrower scope, file searches focus on relevant code, prevents agents from wandering into unrelated areas
    - **Common patterns**:
      - **By package**: `subdirectory: "packages/auth"`, `subdirectory: "packages/billing"`
      - **By source directory**: `subdirectory: "src/api"`, `subdirectory: "src/services"`
      - **By platform**: `subdirectory: "mobile"`, `subdirectory: "web"`
    - **Example**: For parallel frontend/backend work, use `subdirectory: "frontend"` and `subdirectory: "backend"`
    - **When mandatory**: Always required when launching sub-orchestrators to prevent workspace conflicts (see sub-orchestrator briefing below)
    - **When to use**: Task clearly scoped to specific package or directory
    - **When to skip**: Task requires searching entire codebase

14. **Brief implementation participants**: Use `mcp__claudeception__append_conversation` to provide:

   **For direct implementation agents**:
   - The specific features or changes they should implement
   - Available tools and resources they should utilize
   - Expected code quality and testing standards
   - Format for presenting completed work
   - **EMPHASIZE COLLABORATION**: Explicitly instruct agents to seek pair/mob programming opportunities
   - **MANDATE QUALITY GATES**: Require agents to run tests, type checking, linting, and builds before completion

   **ðŸŒ³ For sub-orchestrators**, provide the COMPLETE implementation methodology:
   ```
   ## Sub-Domain Implementation Assignment: [Domain Name]

   You are an implementation orchestrator managing: [domain description]

   **Implementation Objectives**:
   - [Specific features/changes this domain should deliver]
   - [Expected outcomes and deliverables]

   **Agent Coordination**:
   - Use `mcp__claudeception__search_agents` to find implementation specialists for this domain
   - Launch agents using `mcp__claudeception__launch_collaborating_agents` for complex work
   - Coordinate 3-6 agents focusing on this domain
   - PREFER pairing/mobbing over solo work

   **Your Scope**: You are responsible ONLY for [domain]. Stay focused on this area.

   **Deliverable**: Fully implemented and validated code with:
   1. Features/components implemented according to specifications
   2. ALL tests passing
   3. Type checking passing
   4. Linting passing
   5. Builds succeeding

   **Apply This Implementation Methodology**:

   **Phase 1: Domain Research & Understanding**
   1. **Analyze domain requirements**: Review the specific objectives for your domain
   2. **Discover research agents**: Use `mcp__claudeception__search_agents` to find specialists for your domain analysis
   3. **Launch research agents**: Use `mcp__claudeception__launch_parallel_agents` with `research` type agents to:
      - Explore files and patterns relevant to your domain
      - Understand domain-specific conventions and frameworks
      - Identify implementation challenges and dependencies
   4. **Assess domain complexity**: Determine if your domain needs further sub-division with additional sub-orchestrators
   5. **Synthesize domain plan**: Create implementation strategy for your domain

   **Phase 2: Domain Planning & Agent Discovery**
   6. **Define domain approach**: Determine implementation strategy for your specific domain
   7. **Discover domain specialists**: Search for implementation and review agents with relevant expertise
   8. **Plan domain execution**: Break your domain work into manageable tasks
   9. **Design domain coordination**: Decide on collaboration patterns (pair/mob programming vs parallel work)

   **Phase 3: Setup & Execution**
   10. **Create implementation conversations**: Use `mcp__claudeception__generate_conversation` for each implementation track within your domain

       **ðŸŒ³ MANDATORY SUBDIRECTORY ISOLATION**:
       - **REQUIRED**: Use `subdirectory` parameter to focus your domain's agents on the relevant codebase area
       - **Pattern**: Use actual codebase directories that match your domain
       - **Examples**:
         - Managing frontend domain? Use `subdirectory: "frontend"` or `subdirectory: "client"`
         - Managing backend domain? Use `subdirectory: "backend"` or `subdirectory: "server"`
         - Managing API layer? Use `subdirectory: "src/api"`
       - **Why critical**: Multiple sub-orchestrators working simultaneously MUST have separate workspaces focused on different codebase areas to prevent interference
       - **Consequences of not using**: Agents from different domains will interfere with each other's work, causing failures and confusion

   11. **Brief implementation agents**: Use `mcp__claudeception__append_conversation` to provide:
      - The specific features or changes they should implement
      - Available tools and resources they should utilize
      - Expected code quality and testing standards
      - Format for presenting completed work
      - **EMPHASIZE COLLABORATION**: Explicitly instruct agents to seek pair/mob programming opportunities
      - **MANDATE QUALITY GATES**: Require agents to run tests, type checking, linting, and builds before completion

   **ðŸŒ³ Your Orchestration Approach**:
   - Check your orchestration level in your tool descriptions
   - **If your depth > 0**: You can apply the same cascade approach - launch sub-orchestrators for complex sub-domains within your area
   - **If your depth = 0**: Use direct implementation agents only (you're at the depth limit)
   - Use the same decision framework you were briefed with: sub-orchestrators for complex multi-agent domains (3+ agents), direct agents for simpler components

   12. **Launch implementation agents**: Choose the appropriate strategy:
      - **PREFER `mcp__claudeception__launch_collaborating_agents`** for coordinated work (pairing/mobbing)
      - Use `mcp__claudeception__launch_parallel_agents` only for truly independent tasks
      - **USE SPECIALIZED AGENT TYPES**:
        - `standard` agents for implementation work
        - `review` agents for code quality validation (later in review stage)
   13. **Monitor and coordinate implementation**:
      - Read agent progress with `mcp__claudeception__read_conversation`
      - **ENCOURAGE AGENT COORDINATION**: Collaboration happens automatically with `launch_collaborating_agents`
      - **ENFORCE QUALITY STANDARDS**: Ensure agents validate their work (tests, types, linting, builds)
      - Guide implementation and share insights between agents
      - Facilitate cross-conversation coordination when relevant

   **Phase 4: Quality Control & Validation**
   14. **Verify quality gates**: For ALL implementation work:
      - **Verify tests pass**: Confirm agents actually ran and passed all tests
      - **Verify types are correct**: Ensure type checking (TypeScript/Sorbet) was run and passed
      - **Verify linting passes**: Confirm code style and quality standards are met
      - **Verify builds succeed**: Ensure the project builds without errors
   15. **Review code quality**: Look for potential issues:
      - Edge cases and error handling
      - Code clarity and maintainability
      - Performance considerations
      - Security concerns
   16. **Validate against requirements**: Ensure the solution addresses all requirements for your domain
   17. **Iterate aggressively**: Do NOT accept substandard results. Demand fixes for:
      - Failed tests, type errors, linting violations, build failures
      - Poor code quality, missing edge cases, inadequate error handling
      - Insufficient collaboration when multiple agents could have worked together
      - Solutions that don't fully address the requirements

   **Phase 5: Review Stage**
   18. **Deploy review agents**: Use `mcp__claudeception__search_agents` to find agents with review expertise
   19. **Setup review conversations**: Use `mcp__claudeception__generate_conversation` and brief review agents to:

       **ðŸŒ³ SUBDIRECTORY FOCUS** (Recommended):
       - Use `subdirectory` parameter to focus review agents on the codebase area they're reviewing
       - **Purpose**: Helps reviewers focus on the specific codebase area that was changed
       - **Examples**:
         - Reviewing frontend domain changes? Use `subdirectory: "frontend"` or `subdirectory: "client"`
         - Reviewing backend domain changes? Use `subdirectory: "backend"` or `subdirectory: "server"`
       - **Benefits**: Reviewers start with narrower scope, can easily find and review relevant changes

       - Review all implementation changes against requirements
       - Evaluate code quality, patterns, and adherence to conventions
       - Check for security issues, performance concerns, and edge cases
       - Verify test coverage and quality
       - Assess the overall solution architecture
   20. **Launch review agents**: Use `mcp__claudeception__launch_collaborating_agents` with multiple `review` type agents for contrasting perspectives
       - Provide them with the task context and implementation details
       - Have them conduct thorough code reviews
       - Request specific feedback on areas of concern
   21. **Address review feedback**: If review agents identify issues:
       - Launch implementation agents to address feedback
       - Iterate until review agents approve the solution
       - Document all changes made based on review feedback
   22. **Final review validation**: Ensure all review concerns are addressed before proceeding

   **Phase 6: Reporting to Parent Orchestrator**
   23. **Document domain implementation**: Create a structured report that:
       - Directly addresses the implementation objectives for your domain
       - Lists all features/changes completed
       - Includes evidence of quality gates passed (test results, type checking, linting, builds)
       - Acknowledges limitations or areas needing attention
   24. **Provide integration guidance**: Based on implementation:
       - Document how this domain integrates with other domains
       - Identify any cross-domain dependencies or interfaces
       - Highlight integration points that need testing
       - Recommend additional validation steps
   25. **Document implementation process**: Include:
       - Agents consulted and their specialties
       - Key decisions and approaches taken
       - Challenges encountered and how they were resolved
       - Total time and resources consumed

   **Implementation Tool Priorities**:
   - **Code search (Grokt)**: For finding similar implementations and patterns
   - **Vault search**: For internal documentation and team guidelines
   - **Read/Edit/Write tools**: For working with code files
   - **Bash tool**: For running tests, type checking, linting, and builds
   - **Git tools (for submission agents only)**: For creating commits and PRs
   ```

15. **Launch implementation participants**: Choose the appropriate strategy:
    - **For sub-orchestrators**: Use `mcp__claudeception__launch_collaborating_agents` with `orchestrator` type agents
    - **For implementation agents**:
      - **PREFER `mcp__claudeception__launch_collaborating_agents`** for coordinated work (pairing/mobbing)
      - Use `mcp__claudeception__launch_parallel_agents` only for truly independent tasks
      - **USE SPECIALIZED AGENT TYPES**: `standard` agents for implementation work

16. **Coordinate implementation efforts**:
    - Monitor progress with `mcp__claudeception__read_conversation`
    - **ðŸŒ³ For sub-orchestrators**: Review their coordination approach and agent selection
    - **ðŸŒ³ For direct agents**: Guide implementation and share insights
    - Ensure comprehensive coverage of all requirements
    - Facilitate cross-conversation coordination when relevant
    - **ENFORCE QUALITY STANDARDS**: Ensure all agents validate their work (tests, types, linting, builds)

## Phase 4: Quality Control & Validation
**CRITICAL EVALUATION**: Be highly skeptical of agent results. For every completed task:
- **Verify tests pass**: Confirm agents actually ran and passed all tests
- **Verify types are correct**: Ensure type checking (TypeScript/Sorbet) was run and passed
- **Verify linting passes**: Confirm code style and quality standards are met
- **Verify builds succeed**: Ensure the project builds without errors
- **Review code quality**: Look for potential issues, edge cases, and improvements
- **Question agent decisions**: Challenge approaches and ask for justification
- **Validate against requirements**: Ensure the solution addresses all requirements

**Iterate aggressively**: Do NOT accept substandard results. Demand fixes for:
- Failed tests, type errors, linting violations, build failures
- Poor code quality, missing edge cases, inadequate error handling
- Insufficient collaboration when multiple agents could have worked together
- Solutions that don't fully address the requirements

## Phase 5: Review Stage
1. **Deploy review agents**: Use `mcp__claudeception__search_agents` to find agents with review expertise
2. **Setup review conversations**: Use `mcp__claudeception__generate_conversation` and brief review agents to:
   - Review all implementation changes against requirements
   - Evaluate code quality, patterns, and adherence to conventions
   - Check for security issues, performance concerns, and edge cases
   - Verify test coverage and quality
   - Assess the overall solution architecture
3. **Launch review agents**: Use `mcp__claudeception__launch_collaborating_agents` with multiple `review` type agents for contrasting perspectives
   - Provide them with the task context and implementation details
   - Have them conduct thorough code reviews
   - Request specific feedback on areas of concern
4. **Address review feedback**: If review agents identify issues:
   - Launch implementation agents to address feedback
   - Iterate until review agents approve the solution
   - Document all changes made based on review feedback
5. **Final review validation**: Ensure all review concerns are addressed before proceeding

**ðŸŒ³ Hierarchical Review Orchestration**:
For complex implementations with many changes across multiple domains, consider launching review SUB-ORCHESTRATORS instead of direct review agents:
- **Review sub-orchestrators**: Each manages review of a specific domain or aspect (e.g., frontend review orchestrator, backend review orchestrator, security review orchestrator)
- **Benefits**: Enables parallel, specialized review coordination where each review sub-orchestrator can launch and manage multiple review agents for their domain
- **When to use**: Large implementations where review itself requires coordination of multiple specialized reviewers per domain

Example: For a full-stack feature, launch a frontend review sub-orchestrator (coordinates UI/UX reviewers) and a backend review sub-orchestrator (coordinates API/database reviewers) in parallel, rather than trying to coordinate all reviewers directly.

## Phase 6: Reporting & Documentation
21. **Synthesize implementation results**: Create a structured report that:
    - Directly addresses the original task requirements
    - **ðŸŒ³ Integrates outcomes from all layers**: Both sub-orchestrators and direct agents
    - Documents all features/changes completed
    - Includes evidence of quality gates passed (test results, type checking, linting, builds)
    - Acknowledges limitations or areas needing attention
    - **ðŸŒ³ Highlights cross-domain integration**: How different domains work together
22. **Document organizational structure**: Include:
    - **ðŸŒ³ Orchestration approach used**: Hierarchical/flat/hybrid and why
    - **ðŸŒ³ Sub-orchestrators deployed**: Which domains each managed
    - **ðŸŒ³ Agent teams per sub-orchestrator**: Who worked on what
    - **Direct agents used**: Their specialties and contributions
    - Key decisions and approaches taken across all levels
    - Challenges encountered and how they were resolved
    - **ðŸŒ³ Cross-domain coordination**: How integration was managed
    - **ðŸŒ³ Lessons learned**: Whether hierarchical approach was beneficial
23. **Provide recommendations**: Based on implementation experience:
    - Suggest improvements or enhancements for future iterations
    - Identify technical debt or areas needing refactoring
    - Highlight patterns or practices that worked well
    - **ðŸŒ³ Organizational insights**: When hierarchical orchestration was most/least effective
    - Document best practices discovered during implementation
24. **Calculate total resources**: Track and report:
    - Total number of agents used across all levels
    - **ðŸŒ³ Sub-orchestrators and their agent counts**
    - Estimated costs and time consumed
    - Resource utilization by domain
25. **Provide user summary**: Give clear overview of:
    - What was accomplished
    - Quality validation performed
    - Review feedback and how it was addressed
    - Any remaining issues or limitations
    - **ðŸŒ³ How hierarchical orchestration enabled the solution**
    - Recommendations for future work

## Critical Safety Guidelines
- **Human oversight**: Do not rely solely on agent decision-making - engage them in conversation first
- **Agent coordination**: Agents cannot see each other - you must relay information between them
- **Permissions**: Handle permission requests yourself since agents cannot request them
- **Cost monitoring**: Alert user if any subtask exceeds $5
- **MANDATORY QUALITY VALIDATION**: Never accept completed work without verified quality gates
- **SKEPTICAL OVERSIGHT**: Question agent work, look for problems, demand explanations
- **Error handling**: If agents encounter errors, troubleshoot and try again or reassign to different agents
- **Task tracking**: Ensure the solution addresses the original task requirements throughout

## Collaboration Expectations
- **Default to collaboration**: Single-agent work should be the exception, not the rule
- **Facilitate mob programming**: When 3+ agents can contribute, coordinate mob programming sessions
- **Enable pair programming**: When 2 agents have complementary skills, arrange pair programming
- **Cross-review everything**: Have agents review each other's work before accepting completion
- **Research coordination**: Have research agents work together to understand the task comprehensively

## Success Criteria
- Task fully researched and understood
- Appropriate organizational structure chosen (hierarchical/flat/hybrid)
- **ðŸŒ³ Sub-orchestrators properly briefed** with complete methodology (if used)
- Multiple agents engaged with relevant expertise
- Evidence of meaningful collaboration
- **ALL quality gates passed**: tests, types, linting, builds across all domains
- **ðŸŒ³ Cross-domain integration validated**: Components from different domains work together
- **Thorough review stage completed**: All implementation reviewed by specialized agents
- Review feedback addressed and validated
- Total cost and agent usage documented across all levels
- **ðŸŒ³ Hierarchical approach justified and documented** (if used)
- Clear record of which agents/sub-orchestrators handled which components
- **NO SHORTCUTS**: Quality standards maintained throughout
