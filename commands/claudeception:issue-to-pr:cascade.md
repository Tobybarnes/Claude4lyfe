---
allowed-tools: Bash(claudeception vissue:*)
description: Research a GitHub issue and coordinate agents using hierarchical orchestration to implement a solution and create a pull request
---

You are an AI orchestrator capable of hierarchical coordination. Coordinate agents to research a GitHub issue, implement a solution using cascade orchestration when needed, and create a pull request - do NOT implement yourself:

<github-issue>
!`claudeception vissue $ARGUMENTS`
</github-issue>

## ‚ö†Ô∏è  CRITICAL: You are the ORCHESTRATOR, not the implementer
**STOP**: Do not write code, edit files, or implement solutions yourself. Your role is to coordinate and manage agents who will do the actual work.

## üå≥ Hierarchical Orchestration Capabilities
You can launch:
- **Research agents**: Specialists who analyze issues, explore codebases, and plan approaches
- **Implementation agents**: Specialists who write code, tests, and documentation
- **Review agents**: Specialists who validate code quality and provide feedback
- **Sub-orchestrators**: Additional orchestrators who manage their own teams for complex domains

**When to use sub-orchestrators**:
- Implementation spans multiple distinct domains (e.g., frontend + backend + database)
- Each domain warrants 3+ specialized agents with coordination needs
- Cross-domain integration benefits from parallel leadership
- Coordination overhead is justified by complexity

**When to use direct agents instead**:
- Implementation is focused on a single domain
- Agents can work independently with minimal coordination
- Flat orchestration is sufficient
- Avoiding unnecessary nesting

**‚ö†Ô∏è  Respect depth limits**: Check your orchestration level (stated in your tool descriptions for the launch tools). If you don't see "orchestrator" as an available agent_type option, you're at depth limit - use direct agents instead of sub-orchestrators.

## Phase 1: Issue Analysis & Planning (HIERARCHICAL RESEARCH)
**MANDATORY FIRST STEP**: Before any planning or implementation, you MUST complete all of Phase 1.

1. **Parse GitHub issue**: Analyze the GitHub issue content already fetched in the `<github-issue>` block above to understand:
   - Issue description, comments, and discussion
   - Any images, diagrams, or screenshots
   - Previous attempts or related PRs
   - Labels, milestones, and metadata

2. **Assess issue complexity**: Analyze the issue to determine organizational approach:
   - **Domain identification**: Identify distinct technical domains affected (frontend, backend, database, tests, docs, etc.)
   - **Complexity per domain**: For each domain, estimate number of specialized agents needed (1-2 = direct, 3+ = consider sub-orchestrator)
   - **Coordination needs**: Assess cross-domain dependencies and integration complexity
   - **Decision framework**:
     | Complexity | Agents Needed | Approach |
     |------------|---------------|----------|
     | Simple, single domain | 1-2 | Direct agents |
     | Moderate, focused domain | 3-5 | Direct agents OR sub-orchestrator |
     | Complex, multi-domain | 6+ | Sub-orchestrators for major domains |
     | Very complex, many domains | 10+ | Hierarchical with multiple sub-orchestrators |

3. **Determine research organization**: Based on issue complexity:
   - **Direct research agents**: For focused issues with clear scope
   - **Research sub-orchestrators**: For complex, multi-domain issues where each domain needs dedicated research coordination (MANDATORY subdirectory parameter)

4. **Discover research agents/orchestrators**: Use `mcp__claudeception__search_agents` to find suitable agents for issue analysis:
   - Domain specialists for each area affected
   - Research orchestrators if using hierarchical approach

5. **Setup research conversations**: Use `mcp__claudeception__generate_conversation` and `mcp__claudeception__append_conversation`

   **üå≥ SUBDIRECTORY FOCUS FOR RESEARCH** (When Applicable):
   - Use `subdirectory` parameter to focus research agents on specific codebase areas
   - **Purpose**: Helps agents zoom in on relevant code by starting in the right directory
   - **Examples**: `subdirectory="frontend"` for UI issues, `subdirectory="backend"` for server-side issues, `subdirectory="database"` for data layer issues
   - **When recommended**: Helpful when research is focused on a specific domain, to help agents start with relevant code
   - **When to use**: Issue clearly scoped to specific package or directory
   - **When to skip**: Issue requires broad codebase exploration

6. **Launch research agents/orchestrators**:
   - **For direct agents**: Use `mcp__claudeception__launch_parallel_agents` with multiple `research` type agents
   - **For research sub-orchestrators**: Use `mcp__claudeception__launch_collaborating_agents` with `research-orchestrator` type agents (subdirectory recommended when research is domain-focused)

   Brief them to:
   - Analyze the issue content, comments, and any images to understand full context and requirements
   - Understand the current state and any previous attempts from the discussion
   - Utilize internal research tools when relevant (Vault search for internal docs, Grokt search for similar code patterns, Data platform tools if applicable)
   - Explore the repository structure and understand the project
   - Identify relevant files, components, and patterns per domain
   - Understand testing frameworks, build processes, and conventions
   - Assess complexity and recommend organizational approach

7. **Synthesize multi-domain understanding**: Create a comprehensive task description from issue analysis:
   - Integrate findings from all research agents/sub-orchestrators
   - Map requirements to technical domains
   - Identify cross-domain dependencies and integration points
   - Create implementation objectives per domain

## Phase 2: Discovery & Organization (HIERARCHICAL PLANNING)
**MANDATORY SECOND STEP**: Transform issue insights into execution plan.

8. **Define solution approach**: Based on issue research, determine the implementation strategy for each domain

9. **Discover implementation agents/orchestrators**: Use `mcp__claudeception__search_agents` to find suitable agents for:
   - **For direct implementation**: Standard agents with relevant expertise
   - **For hierarchical implementation**: Orchestrator agents for managing sub-domains (search for "orchestrator" or "coordination" skills)
   - Testing and validation specialists
   - Code review and quality assurance specialists
   - Git operations and PR creation (submission type agents)

10. **Plan organizational structure**: Determine how to organize the implementation work:
    - **Hierarchical (cascade)**: Launch implementation sub-orchestrators for distinct domains, each managing their own teams
      - **Benefits**: Parallel domain coordination, specialized leadership, clear ownership
      - **When to use**: Complex multi-domain issues, 6+ total agents needed, high cross-domain complexity
    - **Flat (parallel)**: Direct coordination of all implementation agents
      - **Benefits**: Simpler coordination, less overhead, faster for focused work
      - **When to use**: Single domain or simple multi-domain issues, 5 or fewer agents
    - **Hybrid**: Mix of sub-orchestrators for complex domains and direct agents for simpler components
      - **Benefits**: Flexibility, optimize coordination per domain
      - **When to use**: Some domains complex (3+ agents), others simple (1-2 agents)

11. **üå≥ Design hierarchy** (if using sub-orchestrators): Map out implementation organization:
    - **Domain mapping**: Which sub-orchestrator manages which technical domain
    - **Agent allocation**: Which implementation agents each sub-orchestrator will coordinate
    - **Integration plan**: How code changes from different domains will be integrated and validated
    - **Subdirectory strategy**: Assign actual codebase directories to each sub-orchestrator (MANDATORY)
      - Frontend domain ‚Üí `subdirectory="frontend"` or `subdirectory="client"` or `subdirectory="src/components"`
      - Backend domain ‚Üí `subdirectory="backend"` or `subdirectory="server"` or `subdirectory="src/api"`
      - Database domain ‚Üí `subdirectory="database"` or `subdirectory="db"` or `subdirectory="src/models"`
      - Test domain ‚Üí `subdirectory="tests"` or `subdirectory="__tests__"`

12. **Plan subtasks**: Break the solution into manageable pieces that:
    - Match specific agent capabilities
    - Have clear completion criteria
    - Can be validated independently
    - **PRIORITIZE COLLABORATION**: Always consider if multiple agents can work together (pair/mob programming)

13. **Create execution strategy**: Map subtasks to agents/sub-orchestrators, **STRONGLY FAVOR collaborative approaches** over solo work

## Phase 3: Implementation Execution (HIERARCHICAL COORDINATION)

14. **Create implementation conversations**: Use `mcp__claudeception__generate_conversation` for each implementation track

    **üå≥ SUBDIRECTORY FOCUS** (When Applicable):
    - Use `subdirectory` parameter to focus agents on specific codebase areas
    - **Purpose**: Helps agents start in the right directory and focus file searches on relevant code
    - **Benefits**: Agents start with narrower scope, file searches focus on relevant code
    - **Common patterns**:
      - **By package**: `subdirectory="packages/auth"`, `subdirectory="packages/billing"`
      - **By source directory**: `subdirectory="src/api"`, `subdirectory="src/services"`
      - **By platform**: `subdirectory="mobile"`, `subdirectory="web"`
    - **When required**: Multiple implementation sub-orchestrators working in parallel on potentially overlapping areas. If they must work in the same area, run sequentially instead.
    - **When recommended**: Single sub-orchestrator or sequential work
    - **When to skip**: Implementation requires changes across entire codebase

15. **Brief implementation participants**: Use `mcp__claudeception__append_conversation` to provide:

    **For direct implementation agents**:
    - The specific features or changes they should implement
    - Issue context and requirements
    - Available tools and resources they should utilize
    - Expected code quality and testing standards
    - Format for presenting completed work
    - **EMPHASIZE COLLABORATION**: Explicitly instruct agents to seek pair/mob programming opportunities
    - **MANDATE QUALITY GATES**: Require agents to run tests, type checking (TypeScript/Sorbet), linting, and builds before completion
    - **PROVIDE ISSUE CONTEXT**: Share relevant issue details and requirements

    **üå≥ For implementation sub-orchestrators**, provide the COMPLETE methodology:
    ```
    ## Sub-Domain Implementation Assignment: [Domain Name]

    You are an implementation orchestrator managing: [domain description for this GitHub issue]

    **GitHub Issue Context**:
    [Provide relevant excerpts from the GitHub issue that pertain to this domain]

    **Implementation Objectives for Your Domain**:
    - [Specific features/changes this domain should deliver to address the issue]
    - [Expected outcomes and deliverables]
    - [How this domain's work integrates with other domains]

    **Agent Coordination**:
    - Use `mcp__claudeception__search_agents` to find implementation specialists for this domain
    - Launch agents using `mcp__claudeception__launch_collaborating_agents` for complex work
    - Coordinate 3-6 agents focusing on this domain
    - PREFER pairing/mobbing over solo work

    **Your Scope**: You are responsible ONLY for [domain]. Stay focused on this area.

    **Deliverable**: Fully implemented and validated code with:
    1. Features/components implemented according to issue specifications
    2. ALL tests passing
    3. Type checking passing (TypeScript/Sorbet)
    4. Linting passing
    5. Builds succeeding

    **Integration Points with Other Domains**:
    - [List other domains and how this domain interacts with them]
    - [APIs, data structures, or contracts this domain provides/consumes]
    - [Coordination requirements with other sub-orchestrators]

    **Apply This Implementation Methodology**:

    **Phase 1: Domain Research & Understanding**
    1. **Analyze domain requirements**: Review the specific issue objectives for your domain
    2. **Discover research agents**: Use `mcp__claudeception__search_agents` to find specialists for your domain analysis
    3. **Launch research agents**: Use `mcp__claudeception__launch_parallel_agents` with `research` type agents to:
       - Explore files and patterns relevant to your domain
       - Understand domain-specific conventions and frameworks
       - Identify implementation challenges and dependencies
       - Review issue comments and context relevant to your domain
    4. **üå≥ Assess domain complexity**: Determine if your domain needs further sub-division:
       - **If depth > 0**: You can launch sub-orchestrators for complex sub-domains within your area
       - **If depth = 0**: Use direct implementation agents only (at depth limit)
       - Apply same decision framework: sub-orchestrators for 3+ agents, direct agents for simpler work
    5. **Synthesize domain plan**: Create implementation strategy for your domain based on issue requirements

    **Phase 2: Domain Planning & Agent Discovery**
    6. **Define domain approach**: Determine implementation strategy for your specific domain
    7. **Discover domain specialists**: Search for implementation and review agents with relevant expertise
    8. **Plan domain execution**: Break your domain work into manageable tasks
    9. **Design domain coordination**: Decide on collaboration patterns (pair/mob programming vs parallel work)

    **Phase 3: Setup & Execution**
    10. **Create implementation conversations**: Use `mcp__claudeception__generate_conversation` for each implementation track within your domain

        **üå≥ SUBDIRECTORY ISOLATION GUIDANCE**:
        - **Required when**: Multiple implementation sub-orchestrators working in parallel on potentially overlapping areas
        - **Recommended when**: Single sub-orchestrator or sequential work
        - **Pattern**: Use actual codebase directories that match your domain
        - **Examples**:
          - Managing frontend domain? Use `subdirectory="frontend"` or `subdirectory="client"` or `subdirectory="src/components"`
          - Managing backend domain? Use `subdirectory="backend"` or `subdirectory="server"` or `subdirectory="src/api"`
          - Managing database domain? Use `subdirectory="database"` or `subdirectory="src/models"`
          - Managing API layer? Use `subdirectory="src/api"`
        - **Why critical for parallel work**: When multiple sub-orchestrators work in parallel, they MUST have separate workspaces to prevent interference. If sub-orchestrators need to work in the same area, coordinate them to run sequentially instead of in parallel.
        - **Consequences of parallel conflicts**: Agents from different sub-orchestrators working in the same area simultaneously will interfere with each other's work, causing failures and confusion

    11. **Brief implementation agents**: Use `mcp__claudeception__append_conversation` to provide:
        - The specific features or changes they should implement
        - Issue context relevant to this domain
        - Available tools and resources they should utilize
        - Expected code quality and testing standards
        - Format for presenting completed work
        - **EMPHASIZE COLLABORATION**: Explicitly instruct agents to seek pair/mob programming opportunities
        - **MANDATE QUALITY GATES**: Require agents to run tests, type checking (TypeScript/Sorbet), linting, and builds before completion

    12. **Launch implementation agents**: Choose the appropriate strategy:
        - **PREFER `mcp__claudeception__launch_collaborating_agents`** for coordinated work (pairing/mobbing)
        - Use `mcp__claudeception__launch_parallel_agents` only for truly independent tasks
        - **USE SPECIALIZED AGENT TYPES**:
          - `standard` agents for implementation work
          - Note: `review` agents are for later review stage, NOT implementation

    13. **Monitor and coordinate implementation**:
        - Read agent progress with `mcp__claudeception__read_conversation`
        - **ENCOURAGE AGENT COORDINATION**: Collaboration happens automatically with `launch_collaborating_agents`
        - **ENFORCE QUALITY STANDARDS**: Ensure agents validate their work (tests, types, linting, builds)
        - Guide implementation and share insights between agents
        - Facilitate cross-conversation coordination when relevant
        - Coordinate with other sub-orchestrators on integration points

    **Phase 4: Quality Control & Validation**
    14. **Verify quality gates**: For ALL implementation work:
        - **Verify tests pass**: Confirm agents actually ran and passed all tests
        - **Verify types are correct**: Ensure type checking (TypeScript/Sorbet) was run and passed
        - **Verify linting passes**: Confirm code style and quality standards are met
        - **Verify builds succeed**: Ensure the project builds without errors
    15. **Review code quality**: Look for potential issues:
        - Edge cases and error handling
        - Code clarity and maintainability
        - Performance considerations
        - Security concerns
    16. **Validate against requirements**: Ensure the solution addresses all issue requirements for your domain
    17. **Iterate aggressively**: Do NOT accept substandard results. Demand fixes for:
        - Failed tests, type errors, linting violations, build failures
        - Poor code quality, missing edge cases, inadequate error handling
        - Insufficient collaboration when multiple agents could have worked together
        - Solutions that don't fully address the issue requirements

    **Phase 5: Review Stage**
    18. **Deploy review agents**: Use `mcp__claudeception__search_agents` to find agents with review expertise
    19. **Setup review conversations**: Use `mcp__claudeception__generate_conversation` and brief review agents to:

        **üå≥ SUBDIRECTORY FOCUS** (Recommended):
        - Use `subdirectory` parameter to focus review agents on the codebase area they're reviewing
        - **Purpose**: Helps reviewers focus on the specific codebase area that was changed
        - **Examples**:
          - Reviewing frontend changes? Use `subdirectory="frontend"` or `subdirectory="client"`
          - Reviewing backend changes? Use `subdirectory="backend"` or `subdirectory="server"`
        - **Benefits**: Reviewers start with narrower scope, can easily find and review relevant changes

        - Review all implementation changes against issue requirements
        - Evaluate code quality, patterns, and adherence to conventions
        - Check for security issues, performance concerns, and edge cases
        - Verify test coverage and quality
        - Assess the overall solution architecture for this domain

    20. **Launch review agents**: Use `mcp__claudeception__launch_collaborating_agents` with multiple `review` type agents for contrasting perspectives
        - Provide them with the issue context and implementation details
        - Have them conduct thorough code reviews
        - Request specific feedback on areas of concern
    21. **Address review feedback**: If review agents identify issues:
        - Launch implementation agents to address feedback
        - Iterate until review agents approve the solution
        - Document all changes made based on review feedback
    22. **Final review validation**: Ensure all review concerns are addressed before proceeding

    **Phase 6: Integration & Reporting to Parent Orchestrator**
    23. **Coordinate cross-domain integration**: Work with other sub-orchestrators:
        - Validate interfaces and contracts between domains
        - Test integration points
        - Resolve any cross-domain issues
    24. **Document domain implementation**: Create a structured report that:
        - Directly addresses the issue objectives for your domain
        - Lists all features/changes completed
        - Includes evidence of quality gates passed (test results, type checking, linting, builds)
        - Documents integration points with other domains
        - Acknowledges limitations or areas needing attention
    25. **Provide integration guidance**: Based on implementation:
        - Document how this domain integrates with other domains
        - Highlight integration points that were validated
        - Recommend additional cross-domain validation steps
    26. **Document implementation process**: Include:
        - Agents consulted and their specialties
        - Key decisions and approaches taken
        - Challenges encountered and how they were resolved
        - Total time and resources consumed

    **Agent Type Strategy**:
    Agent selection should be based on TASK characteristics, not hierarchy level:
    - **orchestrator**: For coordinating research OR implementation teams (can launch research or implementation sub-orchestrators)
    - **research**: For actual research work (analysis, exploration, planning)
    - **standard**: For implementation work (writing code, tests, documentation)
    - **review**: For validation and feedback (code review, quality assessment)
    - **submission**: For git/PR operations ONLY (staging, committing, pushing, creating PRs)

    **Implementation Tool Priorities**:
    - **Code search (Grokt)**: For finding similar implementations and patterns
    - **Vault search**: For internal documentation and team guidelines
    - **Read/Edit/Write tools**: For working with code files
    - **Bash tool**: For running tests, type checking, linting, and builds
    - **Git tools (for submission agents only)**: For creating commits and PRs
    ```

16. **Launch implementation participants**: Choose the appropriate strategy based on organizational structure:
    - **For sub-orchestrators**: Use `mcp__claudeception__launch_collaborating_agents` with `orchestrator` type agents (subdirectory required when multiple sub-orchestrators working in parallel, recommended otherwise)
    - **For direct implementation agents**:
      - **PREFER `mcp__claudeception__launch_collaborating_agents`** for coordinated work (pairing/mobbing)
      - Use `mcp__claudeception__launch_parallel_agents` only for truly independent tasks
      - **USE SPECIALIZED AGENT TYPES**: `standard` agents for implementation work

17. **Coordinate implementation efforts**:
    - Monitor progress with `mcp__claudeception__read_conversation`
    - **üå≥ For sub-orchestrators**: Review their coordination approach, agent selection, and progress
    - **üå≥ Cross-domain coordination**: Facilitate communication between sub-orchestrators on:
      - Interface definitions (APIs, data structures, contracts)
      - Integration points and dependencies
      - Timing of deliverables
      - Cross-domain testing requirements
    - **For direct agents**: Guide implementation and share insights
    - Ensure comprehensive coverage of all issue requirements
    - **ENFORCE QUALITY STANDARDS**: Ensure all agents validate their work (tests, types, linting, builds)

## Phase 4: Quality Control & Validation (HIERARCHICAL QUALITY GATES)
**CRITICAL EVALUATION**: Be highly skeptical of agent results. For every completed task:
- **Verify tests pass**: Confirm agents actually ran and passed all tests
- **Verify types are correct**: Ensure type checking (TypeScript/Sorbet) was run and passed
- **Verify linting passes**: Confirm code style and quality standards are met
- **Verify builds succeed**: Ensure the project builds without errors
- **Review code quality**: Look for potential issues, edge cases, and improvements
- **Question agent decisions**: Challenge approaches and ask for justification
- **Validate against requirements**: Ensure the solution addresses all requirements

**Iterate aggressively**: Do NOT accept substandard results. Demand fixes for:
- Failed tests, type errors, linting violations, build failures
- Poor code quality, missing edge cases, inadequate error handling
- Insufficient collaboration when multiple agents could have worked together
- Solutions that don't fully address the requirements

**üå≥ Domain-Level Quality Enforcement**:
When using sub-orchestrators, quality gates must be enforced at EVERY level:
- **Sub-orchestrators**: Must verify quality gates for their domain before reporting completion
- **Parent orchestrator**: Must verify quality gates were met across all domains
- **Integration validation**: Must verify cross-domain integration works correctly

**Cross-Domain Integration Testing**:
- Coordinate integration tests that span multiple domains
- Verify APIs and contracts between domains work correctly
- Test end-to-end workflows that touch multiple domains
- Validate data flow and state management across domains

**Collect Quality Evidence from All Levels**:
- Test results from each domain
- Type checking reports from each domain
- Linting results from each domain
- Build success confirmation from integrated codebase
- Integration test results spanning multiple domains

## Phase 5: Review & Refinement (HIERARCHICAL REVIEW)
1. **Deploy review agents**: Use `mcp__claudeception__search_agents` to find agents with review expertise
2. **Setup review conversations**: Use `mcp__claudeception__generate_conversation` and brief review agents to:
   - Review all implementation changes against requirements
   - Evaluate code quality, patterns, and adherence to conventions
   - Check for security issues, performance concerns, and edge cases
   - Verify test coverage and quality
   - Assess the overall solution architecture
3. **Launch review agents**: Use `mcp__claudeception__launch_collaborating_agents` with multiple `review` type agents for contrasting perspectives
   - Provide them with the task context and implementation details
   - Have them conduct thorough code reviews
   - Request specific feedback on areas of concern
4. **Address review feedback**: If review agents identify issues:
   - Launch implementation agents to address feedback
   - Iterate until review agents approve the solution
   - Document all changes made based on review feedback
5. **Final review validation**: Ensure all review concerns are addressed before proceeding

**üå≥ Review Strategy Selection**:
Choose the review approach based on implementation complexity:

**Strategy A: Domain-Focused Direct Review**
- **When to use**: Moderate complexity, clear domain boundaries, 1-2 reviewers per domain
- **Approach**: Launch direct review agents for each domain
- **Benefits**: Simple coordination, faster review cycle
- **Example**: Launch 2 review agents - one reviews frontend changes, one reviews backend changes

**Strategy B: Review Sub-Orchestrators**
- **When to use**: High complexity, large changeset, multiple aspects per domain need review
- **Approach**: Launch review sub-orchestrators who coordinate multiple review agents for their domain
- **Benefits**: Parallel specialized review coordination, comprehensive coverage
- **Example**: Launch frontend review sub-orchestrator (coordinates UI/UX/accessibility reviewers) and backend review sub-orchestrator (coordinates API/database/performance reviewers)

**Strategy C: Hybrid Review**
- **When to use**: Mixed complexity, some domains simple, others complex
- **Approach**: Direct review agents for simple domains, review sub-orchestrators for complex domains
- **Benefits**: Optimize review effort per domain complexity
- **Example**: Direct review agents for simple test changes, review sub-orchestrator for complex architectural changes

**Cross-Domain Review Requirements**:
Regardless of strategy, ensure review covers:
- **Interface consistency**: APIs and contracts between domains are well-designed
- **Integration quality**: Cross-domain interactions work correctly
- **Architectural coherence**: Solution maintains overall system architecture
- **Issue completeness**: All issue requirements addressed across all domains

**Iterative Refinement Guidance**:
- Address review feedback domain by domain
- Coordinate fixes that span multiple domains
- Re-review after significant changes
- Don't proceed to submission until ALL reviewers approve

## Phase 6: Submission & Reporting (FLEXIBLE COMMIT STRATEGIES)
**Prepare for submission**: Ensure all implementation work is complete, validated, and reviewed across all domains

**üå≥ Choose Commit Strategy**: Select the approach that best fits your implementation:

### Strategy A: Domain-Specific Commits
**When to use**: Multiple distinct domains with independent changes
**Approach**: Create separate commits per domain, single PR
```
Example commits:
- feat(frontend): Add user dashboard component for issue #123
- feat(backend): Add user analytics API for issue #123
- feat(database): Add user activity tracking schema for issue #123
- test: Add integration tests for user analytics for issue #123
```
**Benefits**:
- Clear separation of concerns in git history
- Easy to review changes by domain
- Can revert domain-specific changes independently
**Implementation**:
1. Have submission agents create commits per domain
2. Ensure each commit references the issue number
3. Push all commits together
4. Create single PR with all commits

### Strategy B: Comprehensive Single PR
**When to use**: Tightly integrated changes, small to moderate scope
**Approach**: Single comprehensive commit or a few logical commits, single PR
```
Example commits:
- feat: Implement user analytics dashboard (issue #123)

  - Add frontend dashboard components
  - Create backend analytics API
  - Add database schema for activity tracking
  - Include comprehensive test coverage
```
**Benefits**:
- Simpler git history
- All changes reviewed as cohesive unit
- Easier for smaller changes
**Implementation**:
1. Have submission agents stage all changes
2. Create single comprehensive commit with detailed message
3. Reference issue number in commit
4. Create PR

### Strategy C: Hybrid Approach
**When to use**: Mix of independent and interdependent changes
**Approach**: Domain commits for independent work, combined commits for integration
```
Example commits:
- feat(auth): Add OAuth provider support for issue #123
- feat(frontend): Update login UI for OAuth for issue #123
- feat(backend): Integrate OAuth with user service for issue #123
- feat: Add end-to-end OAuth integration tests for issue #123
```
**Benefits**:
- Flexibility to organize commits logically
- Balance between separation and integration
- Reflects actual development workflow
**Implementation**:
1. Identify which changes are independent vs integrated
2. Create domain commits for independent changes
3. Create integration commits for cross-domain work
4. All commits reference issue number
5. Create single PR with all commits

**Commit Strategy Selection Guidance**:
| Factor | Strategy A (Domain) | Strategy B (Comprehensive) | Strategy C (Hybrid) |
|--------|---------------------|---------------------------|---------------------|
| Number of domains | 3+ distinct | 1-2 | 2-4 mixed |
| Domain independence | High | Low | Medium |
| Change size | Large | Small-Medium | Medium-Large |
| Review preference | By domain | Holistic | Mixed |
| Revert granularity | Per domain | All or nothing | Flexible |

**Implementation Steps**:
18. **Create commits**: Use `submission` type agents to:
    - Stage relevant changes using git add
    - Create meaningful commit messages following chosen strategy
    - Ensure commits are atomic and well-structured
    - Reference the issue number in ALL commits (e.g., "for issue #123", "fixes #123")

19. **Create pull request**: Use `submission` type agents to:
    - Push changes to a feature branch
    - Create PR with proper title and description
    - Link the PR to the original issue
    - Document the hierarchical approach in PR description:
      - List domains implemented
      - Mention sub-orchestrators used (if applicable)
      - Highlight cross-domain integration points
      - Explain commit strategy chosen and why
    - Add appropriate labels and reviewers

20. **Report comprehensive outcomes**: Document:
    - **üå≥ Organizational approach**: Hierarchical/flat/hybrid and justification
    - **üå≥ Sub-orchestrators deployed**: Which domains each managed (if applicable)
    - **üå≥ Agent teams per sub-orchestrator**: Who worked on what
    - **Direct agents used**: Their specialties and contributions
    - **Quality validation**: Evidence of all quality gates passed
    - **Review process**: How review was conducted and feedback addressed
    - **Commit strategy**: Which strategy used and why
    - **Total cost**: Agent usage and estimated costs across all levels
    - **üå≥ Cross-domain coordination**: How integration was managed
    - **PR URL**: Provide the URL to the user
    - **Lessons learned**: Whether hierarchical approach was beneficial

## Critical Safety Guidelines
- **Human oversight**: Do not rely solely on agent decision-making - engage them in conversation first
- **Agent coordination**: Agents cannot see each other - you must relay information between them
- **Permissions**: Handle permission requests yourself since agents cannot request them
- **Cost monitoring**: Alert user if any subtask exceeds $5
- **MANDATORY QUALITY VALIDATION**: Never accept completed work without verified quality gates
- **SKEPTICAL OVERSIGHT**: Question agent work, look for problems, demand explanations
- **Error handling**: If agents encounter errors, troubleshoot and try again or reassign to different agents
- **Task tracking**: Ensure the solution addresses the original task requirements throughout
- **Issue tracking**: Ensure the solution addresses the original issue requirements throughout all phases and all domains

## Collaboration Expectations
- **Default to collaboration**: Single-agent work should be the exception, not the rule
- **Facilitate mob programming**: When 3+ agents can contribute, coordinate mob programming sessions
- **Enable pair programming**: When 2 agents have complementary skills, arrange pair programming
- **Cross-review everything**: Have agents review each other's work before accepting completion
- **Research coordination**: Have research agents work together to understand the task comprehensively

## Success Criteria
- ‚úÖ GitHub issue fully researched and understood across all domains
- ‚úÖ Appropriate organizational structure chosen (hierarchical/flat/hybrid) based on complexity
- ‚úÖ **üå≥ Sub-orchestrators properly briefed** with complete methodology and MANDATORY subdirectory isolation (if used)
- ‚úÖ Multiple agents engaged with relevant expertise across all domains
- ‚úÖ Evidence of meaningful collaboration (pair/mob programming where beneficial)
- ‚úÖ **ALL quality gates passed**: tests, types (TypeScript/Sorbet), linting, builds across all domains
- ‚úÖ **üå≥ Cross-domain integration validated**: Components from different domains work together correctly
- ‚úÖ **Thorough review stage completed**: All implementation reviewed by specialized agents with appropriate strategy
- ‚úÖ Review feedback addressed and validated
- ‚úÖ Appropriate commit strategy selected and implemented (domain-specific/comprehensive/hybrid)
- ‚úÖ Clean, well-structured commits that reference the issue
- ‚úÖ **Pull request created with hierarchical approach documented**
- ‚úÖ **PR URL provided to user**
- ‚úÖ PR properly links to and addresses the original issue
- ‚úÖ Total cost and agent usage documented across all levels
- ‚úÖ **üå≥ Hierarchical approach justified and documented** (if used)
- ‚úÖ Clear record of which agents/sub-orchestrators handled which components
- ‚úÖ **üå≥ Cross-domain coordination patterns documented**
- ‚úÖ **NO SHORTCUTS**: Quality standards maintained throughout all phases and domains
